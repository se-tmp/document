\begin{enumerate}
      \item Purpose\\
            The computer vision part of TempoMate is responsible for the following tasks. Receive video information from the backend server where the cameras connected by the user are connected. Analyze this video information in real time to determine which camera has a person in which position, and what pose the person is in. This data is then analyzed to see if any of the set triggers can be triggered. Once it has determined which triggers to fire, it sends a message through the backend to the specific client about which triggers have been fired and what routines need to be triggered.\\\\

      \item Funtionality\\
            \begin{enumerate}
                  \item Camera connection:\\
                        Since Matter does not support cameras, we need to receive video directly from the user's connected camera via another protocol. We send the video from the user's camera to the backend media server via RTMP and get the camera video from the backend media server via the same protocol.\\

                  \item Pose recognition:\\
                        After fetching the camera footage in real-time, we analyze the camera's screen at 30 frames per second to find out if a person is present and, if so, in what position. The data found is represented by a dot on the top left of the box representing the person, a dot on the bottom right, and finally a pose.\\

                  \item Normalize the data:\\
                        Since different cameras have different resolutions, we represent the coordinates as floating-point points corresponding to 0-1. Also, since people can move, we cluster data in similar locations. Since there is a possibility of model malfunction, we filter out the 30 frames per second that we recognize more than a certain number of times.\\

                  \item Enable triggers:\\
                        Using the normalized data and the trigger data received through the backend, we analyze if there are any triggers that can be activated. If a person's pose remains in one position, it is handled appropriately to prevent multiple triggers.\\\\
            \end{enumerate}
            
      \item Location of source code \\
            : www.github.com/se-tmp/backend \\\\

      \item Class Component\\
            \begin{enumerate}
                  \item processing\\
                        The contents of this folder are for services that act like a backend to process the information received from the camera.\\

                  \begin{description}
                        \item \textbf{model.pt}\\
                            This file is the most important part of this part, given a specific frame, it determines if a person is in the frame, and if so, it outputs a rectangle with LeftTop and RightBottom points, as well as the pose the person is in, and what the confidence level is for this analysis. This tells us where the person is in the frame from the camera and what pose they are in.\\

                        \item \textbf{ThreadedCamera.py}\\
                            This class is instantiated in main.py with the address of a backend media server that can be reached via the RTMP protocol. This class decides how many milliseconds to get frames back, and starts a thread to get them back at the determined time. It also decides how many frames to use as a buffer.\\

                        \item \textbf{Processor.py}\\
                            This class is instantiated in main.py, and at certain times thereafter, it starts a thread to process the data it receives from main.py and stores.\\
                            Each piece of data is given in the form of (LeftTop point, RightBottom point, posture). We cluster these data together if they are located at similar coordinates. The criteria for similarity is that if the Euclidean distance of each coordinate is less than or equal to a certain threshold value, they belong to the same group. Also, since each object in each group is analyzed from a single frame, size of the group means how many frame is detected, if the size of the group is less than a certain number, it is considered to be incorrectly analyzed and ignored. Next, the center point of the coordinates in this group is found and used as a representative value for this group. If there are any triggers fetched from the backend that can be activated using this representative value, we activate them.\\
                            To avoid firing the same trigger over and over again because the person doesn't change their posture, we save the results from the previous analysis and don't activate the trigger if the results from the previous analysis are the same as the results from the current analysis.\\

                        \item \textbf{main.py}\\
                            This file creates a Camera object using the ThreadedCamera class and information about the input camera. It also creates a Processor object.\\ 
                            For the created camera object, it receives a frame at a predetermined frame time and analyzes the frame using a pre-trained model. The result of the analysis is the coordinates of the vertices of the rectangle within the camera frame and, if a person is present, what part of the frame they are in and what pose they are in. The resulting coordinates are divided by a predetermined frame size, normalized to a floating point between 0 and 1, and passed to the Processor class. If the camera is disconnected, it will try to reconnect at a specified time and if it does, it will repeat the same operation.\\
                  \end{description}

      \item \textbf{training/train.ipynb}\\
            This file is the code to train the YOLOv8 model on the dataset we created.\\
            We feed the dataset into the model, and if the results we get from the model are wrong, we back-propagate to modify the model's neural network values. This process gets the model closer to being a model that can output the results we want.\\
    \end{enumerate}
\end{enumerate}